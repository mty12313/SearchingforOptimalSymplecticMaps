# Searching for Optimal Symplectic Maps

This repository contains code and supporting materials for a research project conducted in the **Illinois Mathematics Lab (IML)** at the **University of Illinois Urbana-Champaign**, under the supervision of **Prof. Ely Kerman**.  

The project focuses on **learning and optimizing symplectic maps** for **symplectic embedding problems** in both **2D and 4D**, using polynomial models, neural networks, and symplectic numerical integration.

---

## Project Overview

A smooth map  
\[
F : \mathbb{R}^{2n} \to \mathbb{R}^{2n}
\]
is **symplectic** if it preserves the canonical symplectic form. Symplectic maps naturally arise as time-1 flows of Hamiltonian systems and are central objects in symplectic geometry.

The main goal of this project is:

> **Given a symplectic domain \(D\), find a symplectic map that embeds \(D\) into the smallest possible symplectic ball.**

This computational problem is closely related to classical theoretical results such as **Gromov’s non-squeezing theorem** and known sharp embeddings for ellipsoids, polydisks, and Lagrangian tori.

---

## Methodology

### 1. Parameterizing Symplectic Maps

To make the problem computationally tractable, we restrict to finite-dimensional families of symplectic maps, including:
- **Hénon-type polynomial symplectic maps**
- **Hamiltonian flows** generated by:
  - Polynomial Hamiltonians
  - Neural-network Hamiltonians

By a result of **Turaev**, any symplectic map on \(\mathbb{R}^{2n}\) can be approximated by compositions of such maps.

---

### 2. Symplectic Integration

To guarantee symplecticity at the numerical level, we use **symplectic integrators**, primarily:
- **Leapfrog (Störmer–Verlet) integration**

This ensures that the learned maps remain symplectic **regardless of discretization error**, which is essential for correctness.

---

### 3. Optimization Framework

The learning pipeline is:
1. Sample points from the boundary of a target domain
2. Apply a parameterized symplectic map
3. Minimize a loss measuring how tightly the image fits inside a symplectic ball

Loss functions include:
- Maximum-radius loss
- Log-sum-exp (LSE) smoothing for stable gradients in 4D

Optimization is performed using the **Adam optimizer**, implemented in **JAX** or **PyTorch**.

---

## Experiments

### 2D Experiments
- Non-convex domains (e.g. keyhole shapes)
- Multiple disjoint circles
- Area-preserving embeddings into nearly optimal disks

### 4D Experiments
Domains studied include:
- Ellipsoids \(E(1,a)\)
- Polydisks \(P(1,a)\)
- Lagrangian tori \(L(1,a)\)

Results are compared against known theoretical capacities (e.g. McDuff–Schlenk, Hind–Opshtein).

---

## Repository Structure

```text
.
├── 2D:4D_NpEmb.py
│   NumPy-based experiments for 2D and 4D symplectic embeddings
│
├── 4D_embedding_JAX.py
│   JAX implementation of 4D symplectic optimization with autodiff
│
├── 4D_leapfrog_PyTorch.py
│   PyTorch implementation using leapfrog (symplectic) integration
│
├── 4D_NNSigmoid.py
│   Neural-network Hamiltonians (sigmoid / tanh activations)
│
├── IML_Fall2025_midterm_pre.pdf
│   Midterm presentation slides (IML)
│
├── IML_Fall2025_poster.pdf
│   Final research poster (IML)
│
└── README.md
